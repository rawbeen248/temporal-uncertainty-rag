# Temporal Uncertainty Tracking in Conversational RAG

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

Official implementation of **"Temporal Uncertainty Tracking in Conversational RAG: Learning to Route Multi-Turn Queries Through Uncertainty Evolution"**

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Dataset Preparation](#dataset-preparation)
- [Training](#training)
- [Evaluation](#evaluation)
- [Reproducing Results](#reproducing-results)
- [Project Structure](#project-structure)
- [Citation](#citation)

## ğŸ¯ Overview

This repository implements the first systematic study of how epistemic and aleatoric uncertainty evolve across conversation turns in RAG systems. Key contributions:

1. **Novel Temporal Metrics**: Uncertainty Decay Rate (UDR), Epistemic Convergence Speed (ECS), Routing Adaptation Score (RAS)
2. **Conversation-Aware Routing**: Adaptive routing based on uncertainty evolution patterns
3. **Comprehensive Evaluation**: Experiments on CoQA and QuAC datasets
4. **Full Reproducibility**: All code, data processing, and evaluation scripts included

## ğŸš€ Installation

### Prerequisites
- Python 3.8 or higher
- CUDA-capable GPU (recommended, 8GB+ VRAM)
- 16GB+ RAM

### Setup

```bash
# Clone the repository
git clone https://github.com/yourusername/temporal-uncertainty-rag.git
cd temporal-uncertainty-rag

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Download NLTK data (required for evaluation)
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords')"
```

## âš¡ Quick Start

```python
from src.models.temporal_router import TemporalUncertaintyRouter
from src.data.dataloader import ConversationDataLoader

# Load data
data_loader = ConversationDataLoader(dataset_name='coqa')
train_data, val_data = data_loader.load_and_preprocess()

# Initialize model
model = TemporalUncertaintyRouter(
    embedding_dim=768,
    hidden_dim=256,
    num_sources=4
)

# Train
from src.training.trainer import Trainer
trainer = Trainer(model, train_data, val_data)
trainer.train(epochs=10)

# Evaluate
from src.evaluation.evaluator import TemporalEvaluator
evaluator = TemporalEvaluator(model, val_data)
results = evaluator.evaluate()
print(results)
```

## ğŸ“Š Dataset Preparation

The code automatically downloads and preprocesses datasets from HuggingFace:

```bash
# Prepare CoQA dataset (recommended)
python scripts/prepare_data.py --dataset coqa --output_dir data/processed/coqa

# Note: QuAC dataset is currently unavailable due to HuggingFace deprecating
# dataset loading scripts. Only CoQA is supported at this time.
```

Datasets used:
- **CoQA**: `stanfordnlp/coqa` (7,199 examples, supports conversational QA)
- **QuAC**: Currently unavailable due to HuggingFace deprecating dataset loading scripts

## ğŸ“ Training

### Train Temporal Uncertainty Router

```bash
# Train on CoQA
python scripts/train.py \
    --dataset coqa \
    --model temporal_router \
    --batch_size 32 \
    --epochs 20 \
    --lr 1e-4 \
    --save_dir checkpoints/coqa

# Note: QuAC training currently unavailable
# Only CoQA is supported in the current version
```

### Configuration

Edit `config/config.yaml` to customize:
- Model architecture (LSTM vs Transformer)
- Embedding dimensions
- Training hyperparameters
- Evaluation settings

## ğŸ“ˆ Evaluation

### Run Full Evaluation

```bash
# Evaluate on test set
python scripts/evaluate.py \
    --model_path checkpoints/coqa/best_model.pt \
    --dataset coqa \
    --output_dir results/coqa

# Generate analysis plots
python scripts/analyze_results.py \
    --results_dir results/coqa \
    --output_dir figures/coqa
```

### Reproduce Paper Results

```bash
# Run all experiments from the paper
bash scripts/run_all_experiments.sh

# This will:
# 1. Train all models (Temporal Router + Baselines)
# 2. Evaluate on test sets
# 3. Generate all figures and tables
# 4. Compute statistical significance
```

Expected results (CoQA dataset):
- Data prepared: 3,283 training conversations, 231 validation conversations
- Turn statistics: Mean 11.05 turns (train), 12.10 turns (val)
- Model training in progress - results will be updated after training

## ğŸ“ Project Structure

```
temporal-uncertainty-rag/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/              # Model implementations
â”‚   â”‚   â”œâ”€â”€ temporal_router.py      # Main temporal routing model
â”‚   â”‚   â”œâ”€â”€ uncertainty_estimator.py # Uncertainty estimation
â”‚   â”‚   â”œâ”€â”€ baselines.py            # Baseline models
â”‚   â”‚   â””â”€â”€ components/             # Model components (LSTM, etc.)
â”‚   â”œâ”€â”€ data/                # Data loading and preprocessing
â”‚   â”‚   â”œâ”€â”€ dataloader.py           # Dataset loaders
â”‚   â”‚   â”œâ”€â”€ preprocessor.py         # Data preprocessing
â”‚   â”‚   â””â”€â”€ conversation_format.py  # Conversation formatting
â”‚   â”œâ”€â”€ training/            # Training logic
â”‚   â”‚   â”œâ”€â”€ trainer.py              # Main training loop
â”‚   â”‚   â””â”€â”€ losses.py               # Custom loss functions
â”‚   â”œâ”€â”€ evaluation/          # Evaluation and metrics
â”‚   â”‚   â”œâ”€â”€ evaluator.py            # Main evaluator
â”‚   â”‚   â”œâ”€â”€ metrics.py              # Temporal metrics (UDR, ECS, RAS)
â”‚   â”‚   â””â”€â”€ statistical_tests.py    # Significance testing
â”‚   â””â”€â”€ utils/               # Utility functions
â”‚       â”œâ”€â”€ config.py               # Configuration management
â”‚       â”œâ”€â”€ logger.py               # Logging utilities
â”‚       â””â”€â”€ visualization.py        # Plotting functions
â”œâ”€â”€ scripts/                 # Execution scripts
â”‚   â”œâ”€â”€ prepare_data.py             # Data preparation
â”‚   â”œâ”€â”€ train.py                    # Training script
â”‚   â”œâ”€â”€ evaluate.py                 # Evaluation script
â”‚   â”œâ”€â”€ analyze_results.py          # Result analysis
â”‚   â””â”€â”€ run_all_experiments.sh      # Run full experiment suite
â”œâ”€â”€ config/                  # Configuration files
â”‚   â”œâ”€â”€ config.yaml                 # Main config
â”‚   â””â”€â”€ experiments/                # Experiment-specific configs
â”œâ”€â”€ tests/                   # Unit tests
â”‚   â”œâ”€â”€ test_models.py
â”‚   â”œâ”€â”€ test_data.py
â”‚   â””â”€â”€ test_metrics.py
â”œâ”€â”€ notebooks/               # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_model_analysis.ipynb
â”‚   â””â”€â”€ 03_result_visualization.ipynb
â”œâ”€â”€ data/                    # Data directory (auto-created)
â”œâ”€â”€ checkpoints/             # Model checkpoints (auto-created)
â”œâ”€â”€ results/                 # Evaluation results (auto-created)
â”œâ”€â”€ figures/                 # Generated figures (auto-created)
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md               # This file
```

## ğŸ“Š Key Metrics

The system computes three novel temporal uncertainty metrics:

1. **Uncertainty Decay Rate (UDR)**: Measures how quickly uncertainty decreases across turns
2. **Epistemic Convergence Speed (ECS)**: Tracks convergence of knowledge gaps
3. **Routing Adaptation Score (RAS)**: Quantifies routing strategy adaptation

See `src/evaluation/metrics.py` for implementations.

## ğŸ”¬ Experiments

The paper includes four main research questions:

- **RQ1**: Uncertainty evolution across turns â†’ See `notebooks/02_model_analysis.ipynb`
- **RQ2**: Temporal vs static routing â†’ See `results/*/routing_comparison.csv`
- **RQ3**: Factors causing uncertainty persistence â†’ See `figures/*/uncertainty_factors.pdf`
- **RQ4**: Personalized routing â†’ See `results/*/personalization_analysis.csv`

## ğŸ“ Citation

If you use this code in your research, please cite:

```bibtex
@article{your2025temporal,
  title={Temporal Uncertainty Tracking in Conversational RAG: Learning to Route Multi-Turn Queries Through Uncertainty Evolution},
  author={Your Name and Co-Authors},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2025}
}
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- CoQA dataset: Stanford NLP Group
- QuAC dataset: Allen Institute for AI
- HuggingFace team for the datasets and transformers libraries

## ğŸ“§ Contact

For questions or issues, please:
1. Open an issue on GitHub
2. Contact: your.email@example.com

## ğŸ”„ Updates

- **2025-02-10**: Initial release
- More updates coming soon...

---

**Note**: This is research code. For production use, additional optimization and testing are recommended.
