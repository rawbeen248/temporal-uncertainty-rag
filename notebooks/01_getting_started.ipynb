{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Uncertainty Tracking in Conversational RAG\n",
    "## Getting Started Notebook\n",
    "\n",
    "This notebook demonstrates how to use the Temporal Uncertainty Router for conversational question answering.\n",
    "\n",
    "**Paper**: \"Temporal Uncertainty Tracking in Conversational RAG: Learning to Route Multi-Turn Queries Through Uncertainty Evolution\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (if not already installed)\n",
    "# !pip install -r ../requirements.txt\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Import our modules\n",
    "from src.data.dataloader import ConversationDataLoader, create_dataloaders\n",
    "from src.models.temporal_router import TemporalUncertaintyRouter, ConversationState\n",
    "from src.models.baselines import create_baseline_models\n",
    "from src.evaluation.metrics import (\n",
    "    compute_routing_metrics,\n",
    "    compute_uncertainty_decay_rate,\n",
    "    compute_epistemic_convergence_speed\n",
    ")\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "We'll use the CoQA dataset for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CoQA dataset\n",
    "data_loader = ConversationDataLoader(\n",
    "    dataset_name='coqa',\n",
    "    cache_dir='../data/cache',\n",
    "    min_turns=3,\n",
    "    max_turns=10\n",
    ")\n",
    "\n",
    "train_conversations, val_conversations = data_loader.load_and_preprocess()\n",
    "\n",
    "print(f\"Loaded {len(train_conversations)} training conversations\")\n",
    "print(f\"Loaded {len(val_conversations)} validation conversations\")\n",
    "\n",
    "# Show example conversation\n",
    "example = train_conversations[0]\n",
    "print(f\"\\nExample conversation: {example.conversation_id}\")\n",
    "print(f\"Context: {example.context[:200]}...\")\n",
    "print(f\"\\nNumber of turns: {len(example.turns)}\")\n",
    "for i, turn in enumerate(example.turns[:3]):\n",
    "    print(f\"\\nTurn {i}:\")\n",
    "    print(f\"  Q: {turn.question}\")\n",
    "    print(f\"  A: {turn.answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Model\n",
    "\n",
    "Create the Temporal Uncertainty Router model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "model = TemporalUncertaintyRouter(\n",
    "    encoder_name='bert-base-uncased',\n",
    "    embedding_dim=768,\n",
    "    hidden_dim=256,\n",
    "    num_lstm_layers=2,\n",
    "    num_sources=4,\n",
    "    dropout=0.1,\n",
    "    num_mc_samples=10\n",
    ").to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel initialized!\")\n",
    "print(f\"Total parameters: {num_params:,}\")\n",
    "print(f\"Trainable parameters: {num_trainable:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Single Query Example\n",
    "\n",
    "Let's test the model on a single query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example query\n",
    "context = \"France is a country in Western Europe. Its capital is Paris, which is known for the Eiffel Tower.\"\n",
    "question = \"What is the capital of France?\"\n",
    "\n",
    "# Format input\n",
    "input_text = f\"Context: {context} Question: {question}\"\n",
    "\n",
    "# Tokenize\n",
    "encoding = tokenizer(\n",
    "    input_text,\n",
    "    max_length=512,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ").to(device)\n",
    "\n",
    "# Forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(\n",
    "        input_ids=encoding['input_ids'],\n",
    "        attention_mask=encoding['attention_mask']\n",
    "    )\n",
    "\n",
    "# Display results\n",
    "routing_probs = output['routing_probs'].cpu().numpy()[0]\n",
    "epistemic = output['epistemic_uncertainty'].cpu().numpy()[0, 0]\n",
    "aleatoric = output['aleatoric_uncertainty'].cpu().numpy()[0, 0]\n",
    "\n",
    "sources = ['Internal KB', 'External Search', 'Clarification', 'Multi-source']\n",
    "predicted_route = np.argmax(routing_probs)\n",
    "\n",
    "print(\"Query:\", question)\n",
    "print(f\"\\nPredicted route: {sources[predicted_route]}\")\n",
    "print(f\"Confidence: {routing_probs[predicted_route]:.4f}\")\n",
    "print(f\"\\nRouting probabilities:\")\n",
    "for source, prob in zip(sources, routing_probs):\n",
    "    print(f\"  {source}: {prob:.4f}\")\n",
    "print(f\"\\nUncertainties:\")\n",
    "print(f\"  Epistemic (model uncertainty): {epistemic:.4f}\")\n",
    "print(f\"  Aleatoric (query ambiguity): {aleatoric:.4f}\")\n",
    "print(f\"  Total: {np.sqrt(epistemic**2 + aleatoric**2):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Turn Conversation Example\n",
    "\n",
    "Now let's see how uncertainty evolves across multiple turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create conversation state tracker\n",
    "conv_state = ConversationState(max_history=10)\n",
    "\n",
    "# Conversation turns\n",
    "context = \"The Eiffel Tower is a wrought-iron lattice tower in Paris, France. It was built in 1889 and stands 330 meters tall.\"\n",
    "questions = [\n",
    "    \"Where is the Eiffel Tower located?\",\n",
    "    \"When was it built?\",\n",
    "    \"How tall is it?\",\n",
    "    \"Who designed it?\"\n",
    "]\n",
    "\n",
    "# Track metrics across turns\n",
    "turn_metrics = []\n",
    "\n",
    "model.eval()\n",
    "for turn_id, question in enumerate(questions):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Turn {turn_id}: {question}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Prepare input\n",
    "    input_text = f\"Context: {context} Question: {question}\"\n",
    "    encoding = tokenizer(\n",
    "        input_text,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "    \n",
    "    # Forward pass with history\n",
    "    with torch.no_grad():\n",
    "        output = model(\n",
    "            input_ids=encoding['input_ids'],\n",
    "            attention_mask=encoding['attention_mask'],\n",
    "            history_embeddings=conv_state.get_history_embeddings(),\n",
    "            history_uncertainties=conv_state.get_history_uncertainties()\n",
    "        )\n",
    "    \n",
    "    # Extract metrics\n",
    "    routing_probs = output['routing_probs'].cpu().numpy()[0]\n",
    "    epistemic = output['epistemic_uncertainty'].cpu().numpy()[0, 0]\n",
    "    aleatoric = output['aleatoric_uncertainty'].cpu().numpy()[0, 0]\n",
    "    \n",
    "    temporal_metrics = {\n",
    "        k: v.cpu().numpy()[0, 0] if v.numel() > 0 else 0.0\n",
    "        for k, v in output['temporal_metrics'].items()\n",
    "    }\n",
    "    \n",
    "    # Display\n",
    "    predicted_route = np.argmax(routing_probs)\n",
    "    print(f\"\\nPredicted route: {sources[predicted_route]}\")\n",
    "    print(f\"Epistemic uncertainty: {epistemic:.4f}\")\n",
    "    print(f\"Aleatoric uncertainty: {aleatoric:.4f}\")\n",
    "    \n",
    "    if turn_id > 0:\n",
    "        print(f\"\\nTemporal Metrics:\")\n",
    "        print(f\"  UDR (Uncertainty Decay Rate): {temporal_metrics['udr']:.4f}\")\n",
    "        print(f\"  ECS (Epistemic Convergence Speed): {temporal_metrics['ecs']:.4f}\")\n",
    "    \n",
    "    # Update conversation state\n",
    "    conv_state.update(\n",
    "        embedding=output['query_embedding'].squeeze(0),\n",
    "        epistemic=output['epistemic_uncertainty'].squeeze(0),\n",
    "        aleatoric=output['aleatoric_uncertainty'].squeeze(0),\n",
    "        routing_decision=predicted_route\n",
    "    )\n",
    "    \n",
    "    # Track for visualization\n",
    "    turn_metrics.append({\n",
    "        'turn': turn_id,\n",
    "        'question': question,\n",
    "        'epistemic': epistemic,\n",
    "        'aleatoric': aleatoric,\n",
    "        'route': predicted_route,\n",
    "        **temporal_metrics\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Uncertainty Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(turn_metrics)\n",
    "\n",
    "# Plot uncertainty evolution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Epistemic and Aleatoric uncertainty\n",
    "ax = axes[0, 0]\n",
    "ax.plot(df['turn'], df['epistemic'], marker='o', label='Epistemic', linewidth=2)\n",
    "ax.plot(df['turn'], df['aleatoric'], marker='s', label='Aleatoric', linewidth=2)\n",
    "ax.set_xlabel('Turn')\n",
    "ax.set_ylabel('Uncertainty')\n",
    "ax.set_title('Uncertainty Evolution Across Turns')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Total uncertainty\n",
    "ax = axes[0, 1]\n",
    "total_uncertainty = np.sqrt(df['epistemic']**2 + df['aleatoric']**2)\n",
    "ax.plot(df['turn'], total_uncertainty, marker='D', color='red', linewidth=2)\n",
    "ax.set_xlabel('Turn')\n",
    "ax.set_ylabel('Total Uncertainty')\n",
    "ax.set_title('Total Uncertainty Over Time')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# UDR\n",
    "ax = axes[1, 0]\n",
    "if len(df) > 1:\n",
    "    ax.plot(df['turn'][1:], df['udr'][1:], marker='o', color='green', linewidth=2)\n",
    "ax.set_xlabel('Turn')\n",
    "ax.set_ylabel('UDR')\n",
    "ax.set_title('Uncertainty Decay Rate (UDR)')\n",
    "ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Routing decisions\n",
    "ax = axes[1, 1]\n",
    "colors = ['blue', 'orange', 'red', 'purple']\n",
    "route_colors = [colors[r] for r in df['route']]\n",
    "ax.scatter(df['turn'], df['route'], c=route_colors, s=100)\n",
    "ax.set_xlabel('Turn')\n",
    "ax.set_ylabel('Route')\n",
    "ax.set_yticks([0, 1, 2, 3])\n",
    "ax.set_yticklabels(sources)\n",
    "ax.set_title('Routing Decisions')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('uncertainty_evolution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved to 'uncertainty_evolution.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare with Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline models\n",
    "baselines = create_baseline_models(num_sources=4, device=device)\n",
    "\n",
    "print(\"Baseline models:\")\n",
    "for name in baselines.keys():\n",
    "    print(f\"  - {name}\")\n",
    "\n",
    "# Compare on a sample\n",
    "print(\"\\nExample routing decisions:\")\n",
    "print(f\"Temporal Router: {sources[predicted_route]}\")\n",
    "print(f\"Random: {sources[baselines['random']()]}\")\n",
    "print(f\"Heuristic (turn 2): {sources[baselines['heuristic'](turn_id=2)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Metrics\n",
    "\n",
    "Summary of the novel temporal metrics introduced in this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Novel Temporal Metrics:\\n\")\n",
    "print(\"1. Uncertainty Decay Rate (UDR):\")\n",
    "print(\"   - Measures how quickly uncertainty decreases across turns\")\n",
    "print(\"   - Higher values = faster uncertainty reduction\")\n",
    "print(f\"   - Example value: {df['udr'][1]:.4f}\")\n",
    "print()\n",
    "print(\"2. Epistemic Convergence Speed (ECS):\")\n",
    "print(\"   - Measures how quickly epistemic uncertainty converges to low values\")\n",
    "print(\"   - Higher values = faster convergence\")\n",
    "print(f\"   - Example value: {df['ecs'][1]:.4f}\")\n",
    "print()\n",
    "print(\"3. Routing Adaptation Score (RAS):\")\n",
    "print(\"   - Measures how well routing adapts to uncertainty changes\")\n",
    "print(\"   - Range: 0-1, higher = better adaptation\")\n",
    "print(\"   - Computed over full conversations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Train the model**: Use `scripts/train.py` to train on full dataset\n",
    "2. **Evaluate**: Use `scripts/evaluate.py` for comprehensive evaluation\n",
    "3. **Analyze results**: Check `notebooks/02_model_analysis.ipynb` for detailed analysis\n",
    "4. **Reproduce paper results**: Run `scripts/run_all_experiments.sh`\n",
    "\n",
    "For more information, see the README.md and documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
